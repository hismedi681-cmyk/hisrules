import streamlit as st
import pandas as pd
import numpy as np
import re
from supabase import create_client, Client, ClientOptions
from httpx import Timeout
import httpx 
from sentence_transformers import SentenceTransformer
from streamlit_pdf_viewer import pdf_viewer 

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ë³‘ì› ê·œì • AI ê²€ìƒ‰ê¸°",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- 2. Supabase ë° AI ëª¨ë¸ ì—°ê²° ---
@st.cache_resource
def init_connections():
    try:
        url = st.secrets["supabase"]["url"]
        key = st.secrets["supabase"]["anon_key"]
        
        default_timeout = Timeout(10.0, read=10.0)
        supabase_options = ClientOptions(
            httpx_client=httpx.Client(timeout=default_timeout)
        )
        supabase = create_client(url, key, options=supabase_options)
        ai_model = SentenceTransformer('jhgan/ko-sbert-nli')
        return supabase, ai_model
    except Exception as e:
        st.error(f"âŒ [ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None, None

@st.cache_data(ttl=600)
def load_map_data(_supabase: Client):
    try:
        response = _supabase.table("regulations_map").select(
            "id, ch_name, std_id, std_name, me_id, me_name, pdf_filename, pdf_url"
        ).order("id").execute()
        
        df = pd.DataFrame(response.data)
        if df.empty: 
            st.warning("âš ï¸ 'ì§€ë„' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. admin_sync.pyë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return pd.DataFrame()
        
        def create_sort_key(std_id_str):
            try:
                parts = re.split(r'[.-]', str(std_id_str))
                return tuple(int(p) for p in parts if p.isdigit())
            except ValueError:
                return (0,)
        df['std_sort_key'] = df['std_id'].apply(create_sort_key)
        return df.sort_values(by=['std_sort_key', 'me_id'])
    except Exception as e:
        st.error(f"âŒ [ì˜¤ë¥˜] 'ì§€ë„' ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return pd.DataFrame()

# --- 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

def run_ai_search(query_text, search_mode, _supabase, _model):
    if not query_text: return [], None
    try:
        query_vector = _model.encode(query_text).tolist()
        if search_mode == "[AI] ì œëª©/ë¶„ë¥˜ ê²€ìƒ‰":
            st.session_state.ai_status = "âœ… 'ì œëª©/ë¶„ë¥˜' ê²€ìƒ‰ ì¤‘..."
            response = _supabase.rpc('match_map', {
                'query_vector': query_vector, 'match_threshold': 0.3, 'match_count': 10
            }).execute()
            return response.data, "map"
        else: 
            st.session_state.ai_status = "âœ… 'ë³¸ë¬¸ ì „ì²´' ê²€ìƒ‰ ì¤‘..."
            response = _supabase.rpc('match_chunks_all', {
                'query_vector': query_vector, 'match_threshold': 0.5, 'match_count': 5
            }).execute()
            return response.data, "chunks"
    except Exception as e:
        st.error(f"âŒ [ì˜¤ë¥˜] AI ê²€ìƒ‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return [], None

@st.cache_data(ttl=3600)
def get_pdf_bytes(url: str):
    """ PDF URLì„ ë°›ì•„ ë°”ì´ë„ˆë¦¬(bytes) ë°ì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. """
    try:
        if url.startswith("http://"): url = url.replace("http://", "https://")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = httpx.get(url, headers=headers, timeout=15.0)
        
        if response.status_code == 200:
            return response.content
        else:
            st.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
            return None
    except Exception as e:
        st.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None


# â˜…â˜…â˜… [NEW] JavaScript ìŠ¤í¬ë¡¤ í—¬í¼ í•¨ìˆ˜ ì •ì˜ (í”½ì…€ ì í”„) â˜…â˜…â˜…
def js_scroll_to_page_relative(scroll_index):
    """ PDF ë·°ì–´ì˜ ë‚´ë¶€ ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì•„ì„œ ìƒëŒ€ì  ì¸ë±ìŠ¤ ìœ„ì¹˜ë¡œ ì´ë™ì‹œí‚¤ëŠ” JS ì½”ë“œë¥¼ ì‚½ì…í•©ë‹ˆë‹¤. """
    
    js_code = f"""
    <script>
        function attemptScroll() {{
            const viewer = document.querySelector('.streamlit-container .st-emotion-base:last-child');
            
            if (viewer) {{
                const scrollableContainer = viewer.querySelector('.react-pdf__Document'); 
                const firstPage = viewer.querySelector('.react-pdf__Page'); // ì²« ë²ˆì§¸ í˜ì´ì§€ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.

                if (scrollableContainer && firstPage) {{
                    const pageHeight = firstPage.offsetHeight; // ì²« í˜ì´ì§€ì˜ í”½ì…€ ë†’ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
                    // ìŠ¤í¬ë¡¤ ìœ„ì¹˜ = ì¸ë±ìŠ¤ * ì¸¡ì •ëœ ë†’ì´
                    const scrollAmount = {scroll_index} * pageHeight;
                    
                    scrollableContainer.scrollTop = scrollAmount;
                    console.log('PDF Scrolled to index: {scroll_index}, Height: ' + pageHeight);
                }} else {{
                    // ì»¨í…Œì´ë„ˆ/í˜ì´ì§€ê°€ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ 0.1ì´ˆ ë’¤ì— ì¬ì‹œë„
                    setTimeout(attemptScroll, 100); 
                }}
            }}
        }}

        // í˜ì´ì§€ê°€ ë¡œë“œëœ í›„ ìŠ¤í¬ë¡¤ ì‹œë„ (0.5ì´ˆ ëŒ€ê¸°)
        setTimeout(attemptScroll, 500); 
    </script>
    """
    st.markdown(js_code, unsafe_allow_html=True)


# â˜…â˜…â˜… [NEW] ìµœì¢… ì•ˆì •í™” ë·°ì–´ í•¨ìˆ˜: ë“€ì–¼ ëª¨ë“œ (ì „ì²´/ë§¥ë½) â˜…â˜…â˜…
def render_pdf_viewer_mode(pdf_url: str, page: int = 1):
    """ 
    [ë“€ì–¼ ëª¨ë“œ] target_pageì— ë”°ë¼ ë¡œë“œ ë°©ì‹ì„ ê²°ì •í•˜ê³ , AI ê²€ìƒ‰ ì‹œ í”½ì…€ ì í”„ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
    """
    # 1. ì…ë ¥ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í™•ì‹¤í•˜ê²Œ intë¡œ ë³€í™˜ (TypeError ë°©ì§€)
    target_page = int(page) 
    
    if not pdf_url:
        st.info("ê·œì •ì„ ì„ íƒí•˜ì„¸ìš”.")
        return

    # 2. ë¡œë”© ëª¨ë“œ ê²°ì • ë° í˜ì´ì§€ ê³„ì‚°
    if target_page == 1:
        # ì¼ë°˜ ê·œì • ëª©ë¡ ë˜ëŠ” í•©ë³¸ PDF í´ë¦­ ì‹œ: ì „ì²´ ë¡œë“œ ì‹œë„
        pages_to_load = [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ëŠ” ì „ì²´ ë¡œë“œ íš¨ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.
        spinner_text = "ğŸ“„ ì „ì²´ ë¬¸ì„œë¥¼ ë¡œë”© ì¤‘..."
        jump_index = 0 # ì í”„ ë¶ˆí•„ìš”
    else:
        # AI ê²€ìƒ‰ ê²°ê³¼ í´ë¦­ ì‹œ: ë§¥ë½ ì°½ ë¡œë“œ (Â±20 í˜ì´ì§€)
        context_range = 20 
        start = int(max(1, target_page - context_range))
        end = int(target_page + context_range)
        
        pages_to_load = list(range(start, end + 1))
        
        # â˜…â˜…â˜… ì í”„ ì¸ë±ìŠ¤ ê³„ì‚°: íƒ€ê²Ÿ í˜ì´ì§€ê°€ ë¡œë“œëœ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ ë‚´ì—ì„œ ëª‡ ë²ˆì§¸ ì¸ë±ìŠ¤ì¸ì§€ ê³„ì‚° â˜…â˜…â˜…
        # (ì˜ˆ: start=30, target_page=50. ì¸ë±ìŠ¤ = 50 - 30 = 20)
        jump_index = target_page - start
        spinner_text = f"ğŸ“„ AI ê²€ìƒ‰ ë¬¸ë§¥ ì°½ ({start}p ~ {end}p) ë¡œë”© ë° {target_page}pë¡œ ì í”„ ì¤‘..."

    # 3. PDF ë Œë”ë§
    with st.spinner(spinner_text):
        pdf_data = get_pdf_bytes(pdf_url)
    
    if pdf_data:
        pdf_viewer(
            input=pdf_data, 
            width=700, 
            height=1000,
            pages_to_render=pages_to_load
        )
        
        # 4. ë Œë”ë§ ì„±ê³µ í›„, AI ê²€ìƒ‰ ëª¨ë“œì¼ ë•Œë§Œ JS ìŠ¤í¬ë¡¤ ì‹¤í–‰
        if target_page > 1 and jump_index > 0:
            js_scroll_to_page_relative(jump_index)
            
    else:
        st.error("âŒ PDF ë¬¸ì„œë¥¼ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def set_pdf_url(url: str, page: int):
    st.session_state.current_pdf_url = url
    st.session_state.current_pdf_page = page
    st.session_state.view_mode = "preview" 

# --- 4. UI êµ¬ì„± (ë©”ì¸ ë£¨í”„) ---

# (ë³´ì•ˆ ì²´í¬)
def check_password():
    if "password" not in st.session_state: return
    if st.session_state["password"] == st.secrets["app_security"]["common_password"]:
        st.session_state["is_authenticated"] = True
        del st.session_state["password"]
    else:
        st.error("ë¹„ë°€ë²ˆí˜¸ ì˜¤ë¥˜")

if "is_authenticated" not in st.session_state: st.session_state.is_authenticated = False
if "view_mode" not in st.session_state: st.session_state.view_mode = "preview"
if "current_pdf_url" not in st.session_state: st.session_state.current_pdf_url = None
if "current_pdf_page" not in st.session_state: st.session_state.current_pdf_page = 1
if "ai_status" not in st.session_state: st.session_state.ai_status = ""

if not st.session_state.is_authenticated:
    st.title("ğŸ¥ ë³‘ì› ê·œì • AI ê²€ìƒ‰ê¸°")
    with st.container(border=True):
        st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", on_change=check_password, key="password")
    st.stop()

# (ë©”ì¸ ì•±)
supabase, ai_model = init_connections()
if not supabase or not ai_model: st.stop()
map_data = load_map_data(supabase)

# í•©ë³¸ PDF URL ê°€ì ¸ì˜¤ê¸°
try:
    combined_pdf_url = supabase.storage.from_("regulations").get_public_url("combined_regulations.pdf")
except Exception:
    combined_pdf_url = None

st.title("ğŸ¥ ë³‘ì› ê·œì • AI ê²€ìƒ‰ê¸°")

# (ì „ì²´ í™”ë©´ ëª¨ë“œ)
if st.session_state.view_mode == "fullscreen":
    st.button("ğŸ”™ ëª©ë¡ ë³´ê¸°", on_click=lambda: st.session_state.update(view_mode="preview"), width='stretch')
    if st.session_state.current_pdf_url:
        render_pdf_viewer_mode(st.session_state.current_pdf_url, st.session_state.current_pdf_page)

# (ë¶„í•  í™”ë©´ ëª¨ë“œ)
else:
    col_nav, col_viewer = st.columns([1, 1.5]) 

    with col_nav:
        if combined_pdf_url:
            st.button(
                "ğŸ“‚ [ì „ì²´ í•©ë³¸ ë³´ê¸°]", 
                on_click=set_pdf_url, 
                args=(combined_pdf_url, 1),
                key="btn_combined_pdf",
                width='stretch'
            )
        
        st.divider()
        
        search_mode = st.radio("ëª¨ë“œ", ["[AI] ì œëª©/ë¶„ë¥˜ ê²€ìƒ‰", "[AI] ë³¸ë¬¸ ë‚´ìš© ê²€ìƒ‰", "ì œëª© ê²€ìƒ‰ (í‚¤ì›Œë“œ)"])
        search_query = st.text_input("ê²€ìƒ‰ì–´", placeholder="ì˜ˆ: ë‚™ìƒ")
        
        st.markdown("### ê·œì • ëª©ë¡")
        
        target_df = map_data
        ai_result_type = None
        
        if search_query:
            if "[AI]" in search_mode:
                with st.spinner(st.session_state.ai_status if st.session_state.ai_status else "AI ê²€ìƒ‰ ì¤‘..."):
                    ai_results, ai_result_type = run_ai_search(search_query, search_mode, supabase, ai_model)
                    
                    if not ai_results:
                        st.info("â„¹ï¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        target_df = pd.DataFrame()
                    else:
                        if ai_result_type == "map":
                             ids = [r['id'] for r in ai_results]
                             target_df = map_data[map_data['id'].isin(ids)]
                        elif ai_result_type == "chunks":
                            st.markdown(f"##### ğŸ” '{search_query}' ê´€ë ¨ ë³¸ë¬¸ ê²€ìƒ‰ ê²°ê³¼ ({len(ai_results)}ê±´)")
                            url_map = map_data.drop_duplicates(subset=['pdf_filename'])
                            url_map = pd.Series(url_map.pdf_url.values, index=url_map.pdf_filename).to_dict()

                            for row in ai_results:
                                with st.container(border=True):
                                    c1, c2 = st.columns([4, 1])
                                    c1.markdown(f"**ğŸ“„ {row['pdf_filename']}** (p.{row['page_num']})")
                                    score = row['similarity']
                                    color = "green" if score >= 0.6 else "orange" if score >= 0.5 else "gray"
                                    c2.markdown(f":{color}[**{score:.0%}**]")
                                    
                                    raw_text = row['context_chunk']
                                    clean_text = raw_text.replace("[ë³¸ë¬¸]", "").strip()
                                    if clean_text.startswith("...Å¸"): clean_text = clean_text.replace("...Å¸", "...")
                                    if search_query:
                                        clean_text = clean_text.replace(search_query, f":red[**{search_query}**]")
                                    st.markdown(f"...{clean_text}...")
                                    
                                    pdf_url = url_map.get(row['pdf_filename'])
                                    if pdf_url:
                                        st.button(
                                            "ğŸ‘‰ ì´ í˜ì´ì§€ ë°”ë¡œ ë³´ê¸°",
                                            key=f"btn_chunk_{row['id']}",
                                            on_click=set_pdf_url,
                                            args=(pdf_url, row['page_num']),
                                            use_container_width=True
                                        )
                            target_df = pd.DataFrame()

            elif "í‚¤ì›Œë“œ" in search_mode:
                q = search_query.lower()
                target_df = map_data[map_data['ch_name'].str.lower().str.contains(q) | 
                                     map_data['std_name'].str.lower().str.contains(q) | 
                                     map_data['me_name'].str.lower().str.contains(q)]
                if target_df.empty: st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if not target_df.empty:
            should_expand = True if search_query else False
            
            for ch_name, ch_df in target_df.groupby('ch_name', sort=False):
                with st.expander(f"ğŸ“‚ {ch_name}", expanded=should_expand):
                    for std_name, std_df in ch_df.groupby('std_name', sort=False):
                        std_id = std_df.iloc[0]['std_id']
                        with st.expander(f"ğŸ“™ {std_id} {std_name}", expanded=should_expand):
                            for _, row in std_df.iterrows():
                                st.button(f"ğŸ“„ {row['me_name']}", key=f"btn_{row['id']}", 
                                          on_click=set_pdf_url, args=(row['pdf_url'], 1))

    with col_viewer:
        st.button(
            "â†—ï¸ ì „ì²´ í™”ë©´ìœ¼ë¡œ ë³´ê¸°", 
            on_click=lambda: st.session_state.update(view_mode="fullscreen"), 
            width='stretch',
            disabled=(st.session_state.current_pdf_url is None)
        )
        
        st.divider()

        if st.session_state.current_pdf_url:
            # â˜…â˜…â˜… í•¨ìˆ˜ í˜¸ì¶œ
            render_pdf_viewer_mode(st.session_state.current_pdf_url, st.session_state.current_pdf_page)
        else:
            st.info("ì™¼ìª½ì—ì„œ ê·œì •ì„ ì„ íƒí•˜ì„¸ìš”.")

# --- ê´€ë¦¬ì íŒ¨ë„ ---
if 'is_admin' not in st.session_state: st.session_state.is_admin = False
st.sidebar.title("ê´€ë¦¬ì íŒ¨ë„")
if st.session_state.is_admin:
    st.sidebar.success("ê´€ë¦¬ì ëª¨ë“œ í™œì„±í™”")
    st.sidebar.markdown("---")
    st.sidebar.dataframe(map_data.head())
else:
    admin_pw = st.sidebar.text_input("ê´€ë¦¬ì ì•”í˜¸:", type="password")
    if admin_pw:
        if admin_pw == st.secrets["app_security"]["admin_password"]:
            st.session_state.is_admin = True
            st.rerun()
        else:
            st.sidebar.error("ì•”í˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")




