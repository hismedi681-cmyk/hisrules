import streamlit as st
import pandas as pd
import numpy as np
import re
from supabase import create_client, Client, ClientOptions
from httpx import Timeout
import httpx 
from sentence_transformers import SentenceTransformer
from streamlit_pdf_viewer import pdf_viewer 

# --- 1. í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ë³‘ì› ê·œì • AI ê²€ìƒ‰ê¸°",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- 1-1. ì‚¬ìš©ì ì¸ì¦ í•¨ìˆ˜ ---
def check_password():
    if "password" not in st.session_state: return
    if st.session_state["password"] == st.secrets["app_security"]["common_password"]:
        st.session_state["is_authenticated"] = True
        del st.session_state["password"]
    else:
        st.error("ë¹„ë°€ë²ˆí˜¸ ì˜¤ë¥˜")

# --- 2. Supabase ë° AI ëª¨ë¸ ì—°ê²° ---
@st.cache_resource
def init_connections():
    try:
        url = st.secrets["supabase"]["url"]
        key = st.secrets["supabase"]["anon_key"]
        
        default_timeout = Timeout(10.0, read=10.0)
        supabase_options = ClientOptions(
            httpx_client=httpx.Client(timeout=default_timeout)
        )
        supabase = create_client(url, key, options=supabase_options)
        ai_model = SentenceTransformer('jhgan/ko-sbert-nli')
        return supabase, ai_model
    except Exception as e:
        st.error(f"âŒ [ì˜¤ë¥˜] ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None, None

@st.cache_data(ttl=600)
def load_map_data(_supabase: Client):
    try:
        response = _supabase.table("regulations_map").select(
            "id, ch_name, std_id, std_name, me_id, me_name, pdf_filename, pdf_url"
        ).order("id").execute()
        
        df = pd.DataFrame(response.data)
        if df.empty: 
            st.warning("âš ï¸ 'ì§€ë„' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. admin_sync.pyë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return pd.DataFrame()
        
        def create_sort_key(std_id_str):
            try:
                parts = re.split(r'[.-]', str(std_id_str))
                return tuple(int(p) for p in parts if p.isdigit())
            except ValueError:
                return (0,)
        df['std_sort_key'] = df['std_id'].apply(create_sort_key)
        return df.sort_values(by=['std_sort_key', 'me_id'])
    except Exception as e:
        st.error(f"âŒ [ì˜¤ë¥˜] 'ì§€ë„' ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return pd.DataFrame()

# --- 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

def run_ai_search(query_text, search_mode, _supabase, _model):
    if not query_text: return [], None
    try:
        query_vector = _model.encode(query_text).tolist()
        if search_mode == "[AI] ì œëª©/ë¶„ë¥˜ ê²€ìƒ‰":
            st.session_state.ai_status = "âœ… 'ì œëª©/ë¶„ë¥˜' ê²€ìƒ‰ ì¤‘..."
            response = _supabase.rpc('match_map', {
                'query_vector': query_vector, 'match_threshold': 0.3, 'match_count': 10
            }).execute()
            return response.data, "map"
        else: 
            st.session_state.ai_status = "âœ… 'ë³¸ë¬¸ ì „ì²´' ê²€ìƒ‰ ì¤‘..."
            response = _supabase.rpc('match_chunks_all', {
                'query_vector': query_vector, 'match_threshold': 0.5, 'match_count': 5
            }).execute()
            return response.data, "chunks"
    except Exception as e:
        st.error(f"âŒ [ì˜¤ë¥˜] AI ê²€ìƒ‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return [], None

@st.cache_data(ttl=3600)
def get_pdf_bytes(url: str):
    """ PDF URLì„ ë°›ì•„ ë°”ì´ë„ˆë¦¬(bytes) ë°ì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. """
    try:
        if url.startswith("http://"): url = url.replace("http://", "https://")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = httpx.get(url, headers=headers, timeout=15.0)
        
        if response.status_code == 200:
            return response.content
        else:
            st.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
            return None
    except Exception as e:
        st.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# â˜…â˜…â˜… [ìˆ˜ì •ë¨] ìœ ì¼í•œ set_pdf_url í•¨ìˆ˜ ì •ì˜ (3ê°œ ì¸ì) â˜…â˜…â˜…
def set_pdf_url(url: str, load_mode_page: int, ai_target_page: int):
    """
    load_mode_page: PDF ë·°ì–´ê°€ ë¡œë“œí•  í˜ì´ì§€ ë²ˆí˜¸ (1=ì „ì²´, >1=ë‹¨ì¼ í˜ì´ì§€)
    ai_target_page: AI ê²€ìƒ‰ ê²°ê³¼ì˜ ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ (ì•ˆë‚´ ë©”ì‹œì§€ìš©)
    """
    st.session_state.current_pdf_url = url
    st.session_state.current_pdf_page = load_mode_page 
    st.session_state.ai_target_page = ai_target_page
    st.session_state.view_mode = "preview" 

# â˜…â˜…â˜… ìµœì¢… ìˆ˜ì •ëœ render_pdf_viewer_mode í•¨ìˆ˜ â˜…â˜…â˜…
def render_pdf_viewer_mode(pdf_url: str, page: int = 1):
    """ 
    [ë“€ì–¼ ëª¨ë“œ] target_load_pageì— ë”°ë¼ ë¡œë“œ ë°©ì‹ì„ ê²°ì •í•©ë‹ˆë‹¤.
    - page=1: ì „ì²´ ë¡œë“œ (Full Scroll Mode)
    - page>1: ë‹¨ì¼ í˜ì´ì§€ ë¡œë“œ (Single Page Mode)
    """
    # 1. ì…ë ¥ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í™•ì‹¤í•˜ê²Œ intë¡œ ë³€í™˜
    target_load_page = int(page) 
    # AI ê²€ìƒ‰ ê²°ê³¼ê°€ ì°¾ì€ í˜ì´ì§€ ë²ˆí˜¸ (ì•ˆë‚´ ë©”ì‹œì§€ìš©)
    target_ai_page = st.session_state.get('ai_target_page', 1) 
    
    if not pdf_url:
        st.info("ê·œì •ì„ ì„ íƒí•˜ì„¸ìš”.")
        return

    # 2. ë¡œë”© ëª¨ë“œ ê²°ì • ë° í˜ì´ì§€ ê³„ì‚°
    if target_load_page == 1:
        # ëª¨ë“œ 1: ì „ì²´ ë¡œë“œ (Full Scroll Mode)
        pages_to_load = [] 
        spinner_text = "ğŸ“„ ì „ì²´ ë¬¸ì„œë¥¼ ë¡œë”© ì¤‘..."
        
        # ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥ (ì „ì²´ ë¡œë“œ ëª¨ë“œ)
        if target_ai_page > 1:
             st.info(f"ğŸ“„ ì „ì²´ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ. AI ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” **í˜ì´ì§€ ë²ˆí˜¸ {target_ai_page}**ë¡œ ìŠ¤í¬ë¡¤í•´ì„œ ê°€ì£¼ì„¸ìš”.")
             
    else:
        # ëª¨ë“œ 2: ë‹¨ì¼ í˜ì´ì§€ ë¡œë“œ (Single Page Mode)
        pages_to_load = [target_load_page]
        spinner_text = f"ğŸ“„ AI ê²€ìƒ‰ íƒ€ê²Ÿ ìª½ë§Œ ë¡œë”© ì¤‘..."
        
        # ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥ (ë‹¨ì¼ í˜ì´ì§€ ëª¨ë“œ)
        st.info(f"âš ï¸ í˜„ì¬ ìª½ì€ AI ê²€ìƒ‰ ê²°ê³¼ ë‚´ìš©ë§Œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ë§¥ì„ í™•ì¸í•˜ë ¤ë©´ 'ğŸ“– ì „ì²´ ê·œì • ìŠ¤í¬ë¡¤' ë²„íŠ¼ì„ ì´ìš©í•´ ì£¼ì„¸ìš”.")
        
    # 3. PDF ë Œë”ë§
    with st.spinner(spinner_text):
        pdf_data = get_pdf_bytes(pdf_url)
    
    if pdf_data:
        pdf_viewer(
            input=pdf_data, 
            width=700, 
            height=1000,
            pages_to_render=pages_to_load
        )
    else:
        st.error("âŒ PDF ë¬¸ì„œë¥¼ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# --- 4. UI êµ¬ì„± (ë©”ì¸ ë£¨í”„) ---

# (ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”)
if "is_authenticated" not in st.session_state: st.session_state.is_authenticated = False
if "view_mode" not in st.session_state: st.session_state.view_mode = "preview"
if "current_pdf_url" not in st.session_state: st.session_state.current_pdf_url = None
if "current_pdf_page" not in st.session_state: st.session_state.current_pdf_page = 1
if "ai_target_page" not in st.session_state: st.session_state.ai_target_page = 1 
if "ai_status" not in st.session_state: st.session_state.ai_status = ""

if not st.session_state.is_authenticated:
    st.title("ğŸ¥ ë³‘ì› ê·œì • AI ê²€ìƒ‰ê¸°")
    with st.container(border=True):
        st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", on_change=check_password, key="password")
    st.stop()

# (ë©”ì¸ ì•±)
supabase, ai_model = init_connections()
if not supabase or not ai_model: st.stop()
map_data = load_map_data(supabase)

# í•©ë³¸ PDF URL ê°€ì ¸ì˜¤ê¸°
try:
    combined_pdf_url = supabase.storage.from_("regulations").get_public_url("combined_regulations.pdf")
except Exception:
    combined_pdf_url = None

st.title("ğŸ¥ ë³‘ì› ê·œì • AI ê²€ìƒ‰ê¸°")

# (ì „ì²´ í™”ë©´ ëª¨ë“œ)
if st.session_state.view_mode == "fullscreen":
    st.button("ğŸ”™ ëª©ë¡ ë³´ê¸°", on_click=lambda: st.session_state.update(view_mode="preview"), width='stretch')
    if st.session_state.current_pdf_url:
        render_pdf_viewer_mode(st.session_state.current_pdf_url, st.session_state.current_pdf_page)

# (ë¶„í•  í™”ë©´ ëª¨ë“œ)
else:
    col_nav, col_viewer = st.columns([1, 1.5]) 

    with col_nav:
        if combined_pdf_url:
            st.button(
                "ğŸ“‚ [ì „ì²´ í•©ë³¸ ë³´ê¸°]", 
                on_click=set_pdf_url, 
                # load_mode=1 (ì „ì²´), ai_target=1
                args=(combined_pdf_url, 1, 1), 
                key="btn_combined_pdf",
                width='stretch'
            )
        
        st.divider()
        
        search_mode = st.radio("ëª¨ë“œ", ["[AI] ì œëª©/ë¶„ë¥˜ ê²€ìƒ‰", "[AI] ë³¸ë¬¸ ë‚´ìš© ê²€ìƒ‰", "ì œëª© ê²€ìƒ‰ (í‚¤ì›Œë“œ)"])
        search_query = st.text_input("ê²€ìƒ‰ì–´", placeholder="ì˜ˆ: ë‚™ìƒ")
        
        st.markdown("### ê·œì • ëª©ë¡")
        
        target_df = map_data
        ai_result_type = None
        
        if search_query:
            if "[AI]" in search_mode:
                with st.spinner(st.session_state.ai_status if st.session_state.ai_status else "AI ê²€ìƒ‰ ì¤‘..."):
                    ai_results, ai_result_type = run_ai_search(search_query, search_mode, supabase, ai_model)
                    
                    if not ai_results:
                        st.info("â„¹ï¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        target_df = pd.DataFrame()
                    else:
                        if ai_result_type == "map":
                             ids = [r['id'] for r in ai_results]
                             target_df = map_data[map_data['id'].isin(ids)]
                        elif ai_result_type == "chunks":
                            st.markdown(f"##### ğŸ” '{search_query}' ê´€ë ¨ ë³¸ë¬¸ ê²€ìƒ‰ ê²°ê³¼ ({len(ai_results)}ê±´)")
                            url_map = map_data.drop_duplicates(subset=['pdf_filename'])
                            url_map = pd.Series(url_map.pdf_url.values, index=url_map.pdf_filename).to_dict()

                            # ë©”íƒ€ë°ì´í„° ì œê±°ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
                            keywords_to_remove = ['[ì„¹ì…˜:', '[í•˜ìœ„ì„¹ì…˜:', '[ê·œì¹™:', '[í–‰ìœ„:', '[ëŒ€ìƒ:'] 

                            for row in ai_results:
                                with st.container(border=True):
                                    c1, c2 = st.columns([4, 1])
                                    # í˜ì´ì§€ ë²ˆí˜¸ë¡œ í‘œì‹œ (ì‚¬ìš©ìê°€ ì¶”í›„ PDFì— ë²ˆí˜¸ë¥¼ ì¶”ê°€í•  ê²ƒì„ ê°€ì •)
                                    c1.markdown(f"**ğŸ“„ {row['pdf_filename']}** (í˜ì´ì§€ ë²ˆí˜¸: {row['page_num']})") 
                                    score = row['similarity']
                                    color = "green" if score >= 0.6 else "orange" if score >= 0.5 else "gray"
                                    c2.markdown(f":{color}[**{score:.0%}**]")
                                    
                                    raw_text = row['context_chunk']
                                    
                                    # ë©”íƒ€ë°ì´í„° ì œê±° ë¡œì§ ì ìš©
                                    for keyword in keywords_to_remove:
                                        raw_text = re.sub(rf'{re.escape(keyword)}[^\]]*\]', '', raw_text)
                                        
                                    clean_text = raw_text.replace("[ë³¸ë¬¸]", "").strip()
                                    if clean_text.startswith("...Å¸"): clean_text = clean_text.replace("...Å¸", "...")
                                    
                                    if search_query:
                                        clean_text = clean_text.replace(search_query, f":red[**{search_query}**]")
                                    st.markdown(f"...{clean_text}...")
                                    
                                    pdf_url = url_map.get(row['pdf_filename'])
                                    if pdf_url:
                                        # 3-1. â˜…â˜…â˜… ë‹¨ì¼ í˜ì´ì§€ ë³´ê¸° ë²„íŠ¼ â˜…â˜…â˜…
                                        st.button(
                                            "ğŸ” ì´ ìª½ë§Œ ë³´ê¸°",
                                            key=f"btn_chunk_single_{row['id']}",
                                            on_click=set_pdf_url,
                                            # load_mode_page = row['page_num'] (ë‹¨ì¼ í˜ì´ì§€ ëª¨ë“œ í™œì„±í™”)
                                            args=(pdf_url, row['page_num'], row['page_num']), 
                                            use_container_width=True
                                        )
                                        # 3-2. â˜…â˜…â˜… ì „ì²´ ê·œì • ìŠ¤í¬ë¡¤ ë²„íŠ¼ â˜…â˜…â˜…
                                        st.button(
                                            "ğŸ“– ì „ì²´ ê·œì • ìŠ¤í¬ë¡¤ (í˜ì´ì§€ ì•ˆë‚´)",
                                            key=f"btn_chunk_full_{row['id']}",
                                            on_click=set_pdf_url,
                                            # load_mode_page = 1 (ì „ì²´ ë¡œë“œ ëª¨ë“œ í™œì„±í™”)
                                            args=(pdf_url, 1, row['page_num']),
                                            use_container_width=True
                                        )
                            target_df = pd.DataFrame()

            elif "í‚¤ì›Œë“œ" in search_mode:
                q = search_query.lower()
                target_df = map_data[map_data['ch_name'].str.lower().str.contains(q) | 
                                     map_data['std_name'].str.lower().str.contains(q) | 
                                     map_data['me_name'].str.lower().str.contains(q)]
                if target_df.empty: st.info("ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if not target_df.empty:
            should_expand = True if search_query else False
            
            for ch_name, ch_df in target_df.groupby('ch_name', sort=False):
                with st.expander(f"ğŸ“‚ {ch_name}", expanded=should_expand):
                    for std_name, std_df in ch_df.groupby('std_name', sort=False):
                        std_id = std_df.iloc[0]['std_id']
                        with st.expander(f"ğŸ“™ {std_id} {std_name}", expanded=should_expand):
                            for _, row in std_df.iterrows():
                                st.button(f"ğŸ“„ {row['me_name']}", key=f"btn_{row['id']}", 
                                          on_click=set_pdf_url, 
                                          # load_mode=1 (ì „ì²´), ai_target=1
                                          args=(row['pdf_url'], 1, 1)) 

    with col_viewer:
        st.button(
            "â†—ï¸ ì „ì²´ í™”ë©´ìœ¼ë¡œ ë³´ê¸°", 
            on_click=lambda: st.session_state.update(view_mode="fullscreen"), 
            width='stretch',
            disabled=(st.session_state.current_pdf_url is None)
        )
        
        st.divider()

        if st.session_state.current_pdf_url:
            # â˜…â˜…â˜… í•¨ìˆ˜ í˜¸ì¶œ
            render_pdf_viewer_mode(st.session_state.current_pdf_url, st.session_state.current_pdf_page)
        else:
            st.info("ì™¼ìª½ì—ì„œ ê·œì •ì„ ì„ íƒí•˜ì„¸ìš”.")

# --- ê´€ë¦¬ì íŒ¨ë„ ---
if 'is_admin' not in st.session_state: st.session_state.is_admin = False
st.sidebar.title("ê´€ë¦¬ì íŒ¨ë„")
if st.session_state.is_admin:
    st.sidebar.success("ê´€ë¦¬ì ëª¨ë“œ í™œì„±í™”")
    st.sidebar.markdown("---")
    st.sidebar.dataframe(map_data.head())
else:
    admin_pw = st.sidebar.text_input("ê´€ë¦¬ì ì•”í˜¸:", type="password")
    if admin_pw:
        if admin_pw == st.secrets["app_security"]["admin_password"]:
            st.session_state.is_admin = True
            st.rerun()
        else:
            st.sidebar.error("ì•”í˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
